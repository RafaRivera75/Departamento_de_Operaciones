# -*- coding: utf-8 -*-
"""Proyecto Departamento de Operaciones.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KcAhHr5jHehgRF3UOMaBlqpE0Ehn85CA

# CONTEXTO

En este notebook se presentan los pasos para construir un modelo de clasificación de imágenes de tensores utilizando el conjunto de datos de Rayos X de pecho, la tarea es automatizar el proceso de detección y clasificación de las enfermedades.

Nos han entregado un conjunto de 133 imágenes de 4 clases: COVID-19, PNEUMONIA BACTERIANA, PNEUMONIA VIRICA y NORMAL

#IMPORTAR LIBRERÍAS Y CONJUNTO DE DATOS
"""

# Importar las librerías necesarias

import os
import cv2
import tensorflow as tf
import numpy as np
from tensorflow.keras import layers, optimizers
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Importar el conjunto de datos, los datos se toman de manera local

from google.colab import drive
drive.mount("/content/drive")

# Especificar el directorio del conjunto de entrenamiento

XRay_Directory = "contenido local"

# Listar las carpetas de dicho directorio
os.listdir(XRay_Directory)

# Utilizar el generador de imágenes para generar datos de imágenes de tensores y normalizarlos
# Utilizar el 20% de los datos para la validación cruzada posterior

image_generator = ImageDataGenerator(rescale = 1./255, validation_split = 0.2)

# Genere lotes de 40 imágenes
# El número total de imágenes es 133 * 4 = 532 imágenes
# El entrenamiento usará es 428 (80%) y la validación usará 104 (20%)
# Realizar muestreo aleatoria y cambio del tamaño en las imágenes

train_generator = image_generator.flow_from_directory(batch_size = 40, directory = XRay_Directory, shuffle = True,
                                                      target_size = (256, 256), class_mode = "categorical", subset = "training")

# cargamos las imágenes del directorio de entrenamiento y generamos los lotes de imágenes con sus respectivas etiquetas
#para su uso en el entrenamiento y la validación del modelo

train_generator = image_generator.flow_from_directory(batch_size = 40, directory = XRay_Directory, shuffle = True,
                                                      target_size = (256, 256), class_mode = "categorical", subset = "validation")

# Generar un lote de 40 imágenes y etiquetas
train_images, train_labels = next(train_generator)

train_images.shape  # Verificamos las dimensiones del lote

train_labels.shape

train_labels

# Creamos un diccionario para asignar las etiquetas a las carpetas dadas

label_names = {0: 'COVID-19', 1: 'NORMAL', 2: 'PNEUMONIA VIRICA', 3: 'PNEUMONIA BACTERIANA'}
label_names

"""# VISUALIZACIÓN DEL DATASET"""

# Creamos una matriz de 36 imágenes junto con sus etiquetas correspondientes
L = 6
W = 6

# Crear una figura de subtramas (subplots) con L filas y W columnas, con un tamaño total de (16, 16) pulgadas
fig, axes = plt.subplots(L, W, figsize=(16, 16))

# Aplanar la matriz de subtramas en un arreglo 1D para facilitar el acceso a cada subtrama por índice
axes = axes.ravel()

# Iterar sobre el rango de índices de las subtramas (0 a L * W - 1)
for i in np.arange(0, L * W):
    # Mostrar la i-ésima imagen de entrenamiento en la subtrama correspondiente
    axes[i].imshow(train_images[i])

    # Establecer el título de la subtrama con el nombre de la clase asociada a la i-ésima imagen
    # Utiliza np.argmax(train_labels[i]) para obtener el índice de la clase con mayor probabilidad en el one-hot encoding de la etiqueta
    axes[i].set_title(label_names[np.argmax(train_labels[i])])

    # Desactivar los ejes (axis) para no mostrar los valores de las coordenadas en la visualización
    axes[i].axis('off')

# Ajustar el espaciado horizontal entre las subtramas para mejorar la legibilidad
plt.subplots_adjust(wspace=0.5)

# Mostrar la visualización de las imágenes con sus etiquetas
plt.show()

"""#IMPORTAR EL MODELO CON PESOS PRE ENTRENADOS - MobileNetV2"""

# Crear el modelo, se crea variable basemodel que contiene la instancia del modelo MobileNetV2 creado con estos parámetros.

basemodel = MobileNetV2(weights = 'imagenet', include_top = False, input_tensor = Input(shape=(256, 256, 3)))

basemodel.summary() # Observamos el detalle del modelo

# Verificamos la cantidad de capas del modelo

capas = basemodel.layers
num_layers = len(capas)
print("Número de capas del modelo:", num_layers)

# Congelar el modelo hasta las últimas etapas y llevar a cabo un re-entrenamiento
for layer in basemodel.layers[:-10]:
  layer.trainable = False

"""#CONSTRUIR Y ENTRENAR EL MODELO"""

# Se asigna la salida del modelo base (basemodel) a la variable 'headmodel'.
# Esto contendrá las salidas de la última capa de la CNN pre-entrenada.
headmodel = basemodel.output

# Se aplica una capa de Average Pooling 2D para reducir la dimensión espacial de los mapas de características.
headmodel = AveragePooling2D(pool_size=(4, 4))(headmodel)

# Se aplica una capa de aplanado (Flatten) para convertir los mapas de características 2D en un vector unidimensional.
headmodel = Flatten(name='flatten')(headmodel)

# Se añade una capa totalmente conectada (Dense) con 256 neuronas y función de activación ReLU.
headmodel = Dense(256, activation='relu')(headmodel)

# Se aplica la técnica de Dropout para desactivar aleatoriamente el 30% de las neuronas durante el entrenamiento.
headmodel = Dropout(0.3)(headmodel)

# Se añade otra capa totalmente conectada (Dense) con 128 neuronas y función de activación ReLU.
headmodel = Dense(128, activation='relu')(headmodel)

# Se aplica nuevamente la técnica de Dropout, esta vez con un valor del 20%.
headmodel = Dropout(0.2)(headmodel)

# Se añade la capa de salida con 4 neuronas y función de activación softmax para la clasificación multiclase.
headmodel = Dense(4, activation='softmax')(headmodel)

# Se crea el modelo completo uniendo el modelo base (basemodel) y la nueva arquitectura construida en la parte superior (headmodel).
model = Model(inputs=basemodel.input, outputs=headmodel)

model.summary()

# OBSERVAMOS EL NÚMERO DE CAPAS DEL MODELO
capas = model.layers
num_layers = len(capas)
print("Número de capas del modelo:", num_layers)

# compilamos el  modelo utilizando la función de pérdida de entropía cruzada categórica, el optimizador RMSprop con
# una tasa de aprendizaje de 1e-4 y una regularización mediante decay, y evalúa el rendimiento utilizando la métrica de precisión.

from keras.optimizers import RMSprop

model.compile(loss = 'categorical_crossentropy', optimizer = RMSprop(learning_rate = 1e-4,  decay = 1e-6), metrics = ['accuracy'])

# Usar la parada temprana (early stopping) para salir del entrenamiento si la pérdida en la validación no disminuye
# incluso después de ciertos epochs
earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 20)

# Almacenar el mejor modelo con la menor pérdida en la validación
checkpointer = ModelCheckpoint(filepath = 'weights.hdf5', verbose = 1, save_best_only=True)

# Crear un generador de imágenes para el entrenamiento
train_generator = image_generator.flow_from_directory(
    batch_size=4,                   # Tamaño del lote (batch size) para el entrenamiento
    directory=XRay_Directory,       # Directorio que contiene las imágenes de entrenamiento
    shuffle=True,                   # Mezclar las imágenes antes de cada época de entrenamiento
    target_size=(256, 256),         # Tamaño al que se redimensionarán las imágenes
    class_mode="categorical",       # Modo de codificación de las etiquetas (categorical para one-hot encoding)
    subset="training"               # Utilizar el conjunto de entrenamiento
)

# Crear un generador de imágenes para la validación
val_generator = image_generator.flow_from_directory(
    batch_size=4,                   # Tamaño del lote (batch size) para la validación
    directory=XRay_Directory,       # Directorio que contiene las imágenes de validación
    shuffle=True,                   # Mezclar las imágenes antes de cada época de validación
    target_size=(256, 256),         # Tamaño al que se redimensionarán las imágenes
    class_mode="categorical",       # Modo de codificación de las etiquetas (categorical para one-hot encoding)
    subset="validation"             # Utilizar el conjunto de validación
)

# Entrenar el modelo utilizando el generador de entrenamiento y el generador de validación
# Además, se utilizan callbacks para guardar los pesos del modelo en el mejor punto durante el entrenamiento y
# para detener el entrenamiento tempranamente si no hay mejoras significativas


history = model.fit_generator(
    train_generator,                             # Generador de entrenamiento
    steps_per_epoch=train_generator.n // 4,      # Número de pasos por época (número total de imágenes de entrenamiento // tamaño del lote de entrenamiento)
    epochs=10,                                   # Número de épocas (iteraciones completas sobre todo el conjunto de entrenamiento)
    validation_data=val_generator,               # Generador de validación
    validation_steps=val_generator.n // 4,        # Número de pasos de validación (número total de imágenes de validación // tamaño del lote de validación)
    callbacks=[checkpointer, earlystopping]      # Lista de callbacks que se utilizarán durante el entrenamiento
)

# Aumentamos los epochs a 30 para obtener mejores resultados

history1 = model.fit_generator(train_generator, steps_per_epoch = train_generator.n // 4, epochs = 30,
                              validation_data = val_generator, validation_steps = val_generator.n // 4,
                              callbacks = [checkpointer, earlystopping])

# con 10 epochs:  loss: 0.2714 - accuracy: 0.9065 - val_loss: 0.5164 - val_accuracy: 0.8269

# En este caso observamos que no hubo mejora

"""#EVALUAR EL MODELO ENTRENADO"""

history1.history.keys()   #Observamos las keys del historial

# Realizamos la grafica del primer entrenamiento

plt.plot(history.history['accuracy'])
plt.plot(history.history['loss'])

plt.title('Perdida y Precision en el entrenamiento')
plt.xlabel('Epoch')
plt.ylabel('Precision y Perdida')
plt.legend(['Precision', 'Perdida'])

# Realizamos la grafica del segundo entrenamiento

plt.plot(history1.history['accuracy'])
plt.plot(history1.history['loss'])

plt.title('Perdida y Precision en el entrenamiento')
plt.xlabel('Epoch')
plt.ylabel('Precision y Perdida')
plt.legend(['Precision', 'Perdida'])

#Observamos que tenemos mejores resultados en la validación

# Crear un gráfico de línea que muestra la pérdida en validación durante el entrenamiento del modelo
plt.plot(history1.history['val_loss'])

plt.title('Perdida en validación')
plt.xlabel('Epoch')
plt.ylabel('Perdida')
plt.legend(['Perdida'])

# Crear un gráfico de línea que muestra la precisión en validación durante el entrenamiento del modelo

plt.plot(history1.history['val_accuracy'])

plt.title('Precision en validación')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.legend(['Precision'])

#Utilizamos el modelo entrenado para predecir la etiqueta de la imagen de test

test_directory = "Carpeta local"

# Creamos el modelo de test

test_gen = ImageDataGenerator(rescale = 1./255)
test_generator = test_gen.flow_from_directory(batch_size = 40, directory = test_directory, shuffle = True,
                                              target_size = (256, 256), class_mode = "categorical")

evaluate = model.evaluate_generator(test_generator, steps = test_generator.n // 4, verbose = 1)

print('Precision en la fase de Test:  {}'.format(evaluate[1]))

# Importar las funciones necesarias de scikit-learn
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Inicializar listas para almacenar las predicciones, las clases originales y las imágenes
predictions = []
original = []
image = []

# Recorrer cada carpeta en el directorio de prueba (cada carpeta representa una clase)
for i in range(len(os.listdir(test_directory))):
  # Recorrer cada archivo de imagen dentro de la carpeta actual (clase)
  for item in os.listdir(os.path.join(test_directory, str(i))):
    # Leer la imagen utilizando OpenCV y redimensionarla a 256x256 píxeles
    img = cv2.imread(os.path.join(test_directory, str(i), item))
    img = cv2.resize(img, (256, 256))
    # Agregar la imagen a la lista "image"
    image.append(img)
    # Normalizar los valores de los píxeles de la imagen al rango [0, 1]
    img = img / 255
    # Reorganizar la forma de la imagen para que coincida con el formato de entrada del modelo
    img = img.reshape(-1, 256, 256, 3)
    # Realizar la predicción utilizando el modelo
    predic = model.predict(img)
    # Obtener el índice de la clase con mayor probabilidad como la predicción
    predic = np.argmax(predic)
    # Agregar la predicción y la clase original a las listas correspondientes
    predictions.append(predic)
    original.append(i)

len(original)

score = accuracy_score(original, predictions)

print("Precision del modelo: {:.2f}%".format(score * 100))

# realizamos una comparacion entre las predicciones y las clases originales

L = 10
W = 4

fig, axes = plt.subplots(L, W, figsize = (14, 24))
axes = axes.ravel()

for i in np.arange(0, L * W):
  axes[i].imshow(image[i])
  axes[i].set_title("Pred={}\nVerd={}".format(str(label_names[predictions[i]]), str(label_names[original[i]])))
  axes[i].axis('off')

plt.subplots_adjust(wspace=1.2, hspace=0.8)
plt.show()

# Realizamos una comparación entre las predicciones y las clases originales

print(classification_report(np.asarray(original), np.asarray(predictions)))

"""#Resumen de las predicciones

De la respuesta dada, se puede concluir lo siguiente:

* La clase 0 tiene la mayor precisión (0.83) y recuperación (1.00), lo que indica que el modelo es muy bueno para predecir correctamente esta clase y no se pierde muchos ejemplos positivos de esta clase.

* La clase 2 tiene la menor precisión (0.71) y recuperación (0.50), lo que indica que el modelo tiene más dificultades para predecir correctamente esta clase y se pierde muchos ejemplos positivos de esta clase.

* La precisión promedio ponderada y la puntuación F1 promedio ponderada son ambas 0.77, lo que indica que el modelo tiene un rendimiento general decente en la predicción de todas las clases.

* La precisión general del modelo es 0.78, lo que indica que el modelo predice correctamente el 78% de los ejemplos en el conjunto de validación.


En resumen, el modelo parece tener un buen rendimiento en general, pero tiene más dificultades para predecir correctamente la clase 2 (se pueden realizar pruebas adicionales para mejorar el rendimiento del modelo, lo ideal es tener muchas mas imagenes para mejorar las predicciones).
"""

# Graficamos una matriz de confusión para observar la exactitud del modelo

cm = confusion_matrix(np.asarray(original), np.asarray(predictions))
ax = plt.subplot()
sns.heatmap(cm, annot=True, ax=ax)
ax.set_xlabel('Predicciones')
ax.set_ylabel('Original')
ax.set_title('Matriz de confusión')